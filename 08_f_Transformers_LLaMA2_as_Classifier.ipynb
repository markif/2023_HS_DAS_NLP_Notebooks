{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"400\" src=\"https://www.fhnw.ch/de/++theme++web16theme/assets/media/img/fachhochschule-nordwestschweiz-fhnw-logo.svg\" alt=\"FHNW Logo\">\n",
    "\n",
    "\n",
    "# LLaMA2 as Classifier\n",
    "\n",
    "by Fabian Märki\n",
    "\n",
    "## Summary\n",
    "The aim of this notebook is to show how to load a Large Language Model into a GPU with limited resources and then do some Prompt Engineering in order to use LLaMA2 as a classifier.\n",
    "\n",
    "\n",
    "## Links\n",
    "- [Efficient Transformers: A Survey](https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/)\n",
    "- [The Secret Sauce behind 100K Context Window in LLMs: All Tricks in One Place](https://blog.gopenai.com/how-to-speed-up-llms-and-use-100k-context-window-all-tricks-in-one-place-ffd40577b4c)\n",
    "- [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
    "- [FlashAttention](https://shreyansh26.github.io/post/2023-03-26_flash-attention/) and how to use it with [Huggingface](https://huggingface.co/docs/transformers/perf_infer_gpu_one)\n",
    "\n",
    "- [List of recent Models and their Licence](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=table)\n",
    "- [Huggingface Model Ecosystem](https://huggingface.co/models)\n",
    "\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/markif/2023_HS_DAS_NLP_Notebooks/blob/master/08_f_Transformers_LLaMA2_as_Classifier.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install 'fhnw-nlp-utils>=0.8.0,<0.9.0'\n",
    "\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "from fhnw.nlp.utils.storage import download\n",
    "from fhnw.nlp.utils.system import set_log_level\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "set_log_level()\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure that a GPU is available (see [here](https://www.tutorialspoint.com/google_colab/google_colab_using_free_gpu.htm))!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS name: posix\n",
      "Platform name: Linux\n",
      "Platform release: 6.2.0-33-generic\n",
      "Python version: 3.8.10\n",
      "CPU cores: 6\n",
      "RAM: 31.1GB total and 23.96GB available\n",
      "Tensorflow version: 2.12.0\n",
      "GPU is available\n",
      "GPU is a NVIDIA GeForce RTX 2070 with Max-Q Design with 8192MiB\n"
     ]
    }
   ],
   "source": [
    "from fhnw.nlp.utils.system import system_info\n",
    "print(system_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.41 s, sys: 1.56 s, total: 7.97 s\n",
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "download(\"https://drive.switch.ch/index.php/s/0hE8wO4FbfGIJld/download\", \"data/german_doctor_reviews_tokenized.parq\")\n",
    "data = load_dataframe(\"data/german_doctor_reviews_tokenized.parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(331187, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all neutral sentimens\n",
    "data = data.loc[(data[\"label\"] != \"neutral\")]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Ich bin franzose und bin seit ein paar Wochen ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[ich, bin, franzose, und, bin, seit, ein, paar...</td>\n",
       "      <td>ich bin franzose und bin seit ein paar wochen ...</td>\n",
       "      <td>[franzose, seit, paar, wochen, muenchen, zahn,...</td>\n",
       "      <td>[franzos, seit, paar, woch, muench, ., zahn, s...</td>\n",
       "      <td>[franzose, seit, paar, wochen, muenchen, ., za...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Dieser Arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "      <td>[dieser, arzt, ist, das, unmöglichste, was, mi...</td>\n",
       "      <td>dieser arzt ist das unmöglichste was mir in me...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnen, unfr...</td>\n",
       "      <td>[arzt, unmog, leb, je, begegnet, unfreund, ,, ...</td>\n",
       "      <td>[arzt, unmöglichste, leben, je, begegnet, unfr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hatte akute Beschwerden am Rücken. Herr Magura...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>[hatte, akute, beschwerden, am, rücken, ., her...</td>\n",
       "      <td>hatte akute beschwerden am rücken . herr magur...</td>\n",
       "      <td>[akut, beschwerden, rücken, magura, erste, arz...</td>\n",
       "      <td>[akut, beschwerd, ruck, ., magura, erst, arzt,...</td>\n",
       "      <td>[akute, beschwerden, rücken, ., magura, erste,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_original  rating   \n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...     2.0  \\\n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...     6.0   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...     1.0   \n",
       "\n",
       "                                                text     label  sentiment   \n",
       "0  Ich bin franzose und bin seit ein paar Wochen ...  positive          1  \\\n",
       "1  Dieser Arzt ist das unmöglichste was mir in me...  negative         -1   \n",
       "2  Hatte akute Beschwerden am Rücken. Herr Magura...  positive          1   \n",
       "\n",
       "                                         token_clean   \n",
       "0  [ich, bin, franzose, und, bin, seit, ein, paar...  \\\n",
       "1  [dieser, arzt, ist, das, unmöglichste, was, mi...   \n",
       "2  [hatte, akute, beschwerden, am, rücken, ., her...   \n",
       "\n",
       "                                          text_clean   \n",
       "0  ich bin franzose und bin seit ein paar wochen ...  \\\n",
       "1  dieser arzt ist das unmöglichste was mir in me...   \n",
       "2  hatte akute beschwerden am rücken . herr magur...   \n",
       "\n",
       "                                         token_lemma   \n",
       "0  [franzose, seit, paar, wochen, muenchen, zahn,...  \\\n",
       "1  [arzt, unmöglichste, leben, je, begegnen, unfr...   \n",
       "2  [akut, beschwerden, rücken, magura, erste, arz...   \n",
       "\n",
       "                                          token_stem   \n",
       "0  [franzos, seit, paar, woch, muench, ., zahn, s...  \\\n",
       "1  [arzt, unmog, leb, je, begegnet, unfreund, ,, ...   \n",
       "2  [akut, beschwerd, ruck, ., magura, erst, arzt,...   \n",
       "\n",
       "                               token_clean_stopwords  \n",
       "0  [franzose, seit, paar, wochen, muenchen, ., za...  \n",
       "1  [arzt, unmöglichste, leben, je, begegnet, unfr...  \n",
       "2  [akute, beschwerden, rücken, ., magura, erste,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fhnw.nlp.utils.transformers import get_compute_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"verbose\": True,\n",
    "    \"shuffle\": True,\n",
    "    # modify batch_size in case you experience memory issues\n",
    "    \"batch_size\": 16,\n",
    "    \"X_column_name\": \"text_clean\",\n",
    "    \"y_column_name\": \"label\",\n",
    "    \"y_column_name_prediction\": \"prediction\",\n",
    "    \"compute_device\": get_compute_device(),\n",
    "    \"last_stored_batch\": -1,\n",
    "}\n",
    "\n",
    "compute_device = get_compute_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to take advantage of some recent freatures, we need to ensure that the latest version of the libraries is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install \"optimum>=1.13.2\"\n",
    "!pip install \"bitsandbytes>=0.41.1\"\n",
    "!pip install \"accelerate>=0.23.0\" #git+https://github.com/huggingface/accelerate.git\n",
    "#!pip install \"git+https://github.com/huggingface/accelerate.git\n",
    "!pip install \"transformers>=4.33.2\" #git+https://github.com/huggingface/transformers.git\n",
    "#!pip install \"git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experienced issues with downloading the model. Finally, I downloaded it directly from the Git-Repository (I actually did it on the console but following code might also work)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get install git-lfs\n",
    "#!git lfs install\n",
    "\n",
    "#!git clone \"https://huggingface.co/NousResearch/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model (some need authentication like meta-llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01e0343fd62455dbc7c9a5a3f8325fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# original model but needs access token\n",
    "#transformers_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "#transformers_model_name = \"codellama/CodeLlama-34b-hf\"\n",
    "transformers_model_name = \"Llama-2-7b-hf\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformers_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    transformers_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    #use_auth_token=<your_access_token>,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3829940224"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take advantage of [FlashAttention](https://huggingface.co/docs/transformers/perf_infer_gpu_one) (see also [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttentionLayerBetterTransformer(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is gravity?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(compute_device)\n",
    "\n",
    "model_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is gravity?\n",
      " Hinweis: Die folgende Seite ist nur auf Englisch verfügbar.\n",
      "Gravity is the force that attracts objects to each other. It is the force that keeps the Earth and the other planets in their orbits around the Sun. It is the force that keeps the Moon in its orbit around the Earth.\n",
      "Gravity is a force of attraction. It is the force that attracts objects to each other. It is the force that keeps the Earth and the other planets in their orbits around the Sun. It is the force that keeps the Moon in its orbit around the Earth.\n",
      "Gravity is a force of attraction. It is the force that attracts objects to each other. It is the force that keeps the Earth and the other planets in their orbits around the Sun. It is the force that keeps the Moon in its orbit around the Earth.\n",
      "Gravity is a force of attraction. It is the\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is gravity?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(compute_device)\n",
    "\n",
    "model_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=200,\n",
    "    do_sample=True,\n",
    "    top_p=0.7,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is gravity? It’s not something you can see or feel. nobody has ever been able to photograph it, or to measure it with any accuracy. The best we can do is to infer its presence from the motion of bodies.\n",
      "We know that gravity is what holds the earth in orbit around the sun, and keeps the sun and the planets in their places in the solar system. We know that it is also responsible for holding the planets and moons in orbit around each other.\n",
      "Gravity is the only force in nature that acts at a distance, and the only force that acts on all objects equally. All other forces, such as electricity, magnetism, and inertia, only act on certain objects, and only when those objects are close together.\n",
      "The first scientist to study gravity was Isaac Newton. He was born in England in 1643, and died in 1727. Newton was a very intelligent man, and he was\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
    "\n",
    "Text: {}\n",
    "Sentiment: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
      "\n",
      "Text: ich bin franzose und bin seit ein paar wochen in muenchen . ich hatte zahn schmerzen und mein kollegue hat mir dr mainka empfohlen . ich habe schnell ein termin bekommen , das team war nett und meine schmerzen sind weg ! ! ich bin als angst patient sehr zurieden ! !\n",
      "Sentiment: \n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(data.iloc[0][params[\"X_column_name\"]])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(compute_device)\n",
    "\n",
    "model_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=8,\n",
    "    do_sample=True,\n",
    "    top_p=0.7,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
      "\n",
      "Text: ich bin franzose und bin seit ein paar wochen in muenchen . ich hatte zahn schmerzen und mein kollegue hat mir dr mainka empfohlen . ich habe schnell ein termin bekommen , das team war nett und meine schmerzen sind weg ! ! ich bin als angst patient sehr zurieden ! !\n",
      "Sentiment: 0.133333\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
    "\n",
    "Examples:\n",
    "Text: Dies ist ein guter Arzt.\n",
    "Sentiment: Positive\n",
    "\n",
    "Text: Dies ist ein schlechter Arzt\n",
    "Sentiment: Negative\n",
    "\n",
    "Text: {}\n",
    "Sentiment: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
      "\n",
      "Examples:\n",
      "Text: Dies ist ein guter Arzt.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Dies ist ein schlechter Arzt\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: ich bin franzose und bin seit ein paar wochen in muenchen . ich hatte zahn schmerzen und mein kollegue hat mir dr mainka empfohlen . ich habe schnell ein termin bekommen , das team war nett und meine schmerzen sind weg ! ! ich bin als angst patient sehr zurieden ! !\n",
      "Sentiment: \n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(data.iloc[0][params[\"X_column_name\"]])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(compute_device)\n",
    "\n",
    "model_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=8,\n",
    "    do_sample=True,\n",
    "    top_p=0.7,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
      "\n",
      "Examples:\n",
      "Text: Dies ist ein guter Arzt.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Dies ist ein schlechter Arzt\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: ich bin franzose und bin seit ein paar wochen in muenchen . ich hatte zahn schmerzen und mein kollegue hat mir dr mainka empfohlen . ich habe schnell ein termin bekommen , das team war nett und meine schmerzen sind weg ! ! ich bin als angst patient sehr zurieden ! !\n",
      "Sentiment: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(compute_device)\n",
    "\n",
    "model_output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=4,\n",
    "    do_sample=True,\n",
    "    top_p=0.7,\n",
    "    temperature=1.0,\n",
    "    num_beams=5,\n",
    "    early_stopping=True,\n",
    "    # might be an option to force \"positive\" and \"negative\"\n",
    "    #force_words_ids(List[List[int]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the german text into negative, or positive. Reply with only one word: Positive, or Negative.\n",
      "\n",
      "Examples:\n",
      "Text: Dies ist ein guter Arzt.\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: Dies ist ein schlechter Arzt\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: ich bin franzose und bin seit ein paar wochen in muenchen . ich hatte zahn schmerzen und mein kollegue hat mir dr mainka empfohlen . ich habe schnell ein termin bekommen , das team war nett und meine schmerzen sind weg ! ! ich bin als angst patient sehr zurieden ! !\n",
      "Sentiment:  Positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
